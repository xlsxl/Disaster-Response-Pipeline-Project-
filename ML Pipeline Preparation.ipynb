{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', \"stopwords\",\"maxent_ne_chunker\", \"words\"])\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products      ...        \\\n",
       "0        0      0            0             0                 0      ...         \n",
       "1        0      0            1             0                 0      ...         \n",
       "2        0      0            0             0                 0      ...         \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "database_filename = \"disaster_response_database.db\"\n",
    "engine = create_engine('sqlite:///' + database_filename)\n",
    "table = database_filename.replace(\".db\",\"_table\")\n",
    "\n",
    "df = pd.read_sql_table(table, engine)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (26216,)\n",
      "Shape of y: (26216, 35)\n"
     ]
    }
   ],
   "source": [
    "X = df.message.values\n",
    "y = df.iloc[:,4:].values\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Manual operation example before defining function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If I need a US Visa at this moment, what do I need to do, because I don't have anywhere to sleep, my whole family is at my side.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining text with a row from X data\n",
    "text = X[1000]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if i need a us visa at this moment  what do i need to do  because i don't have anywhere to sleep  my whole family is at my side \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization : \n",
    "# Replace punctuations with \" \" and make string lowercase\n",
    "# Replace all punctuations except apostrophes\n",
    "text = re.sub(\"[^a-zA-Z0-9']\", \" \", text.lower())\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize: Split to words\n",
    "#tokenized = nltk.word_tokenize(text)\n",
    "#print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'i', 'need', 'a', 'us', 'visa', 'at', 'this', 'moment', 'what', 'do', 'i', 'need', 'to', 'do', 'because', 'i', \"don't\", 'have', 'anywhere', 'to', 'sleep', 'my', 'whole', 'family', 'is', 'at', 'my', 'side']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize: Split to words\n",
    "tokenized = text.split(\" \")\n",
    "tokenized = [word for word in tokenized if word != \"\" ]\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['need', 'us', 'visa', 'moment', 'need', 'anywhere', 'sleep', 'whole', 'family', 'side']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop_words\n",
    "cleaned = [word for word in tokenized if word not in stop_words]\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('need', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('visa', 'VB'),\n",
       " ('moment', 'NN'),\n",
       " ('need', 'NN'),\n",
       " ('anywhere', 'RB'),\n",
       " ('sleep', 'JJ'),\n",
       " ('whole', 'JJ'),\n",
       " ('family', 'NN'),\n",
       " ('side', 'NN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of speech tagging\n",
    "tagged = nltk.pos_tag(cleaned)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['need', 'us', 'visa', 'moment', 'need', 'anywhere', 'sleep', 'whole', 'family', 'side']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize\n",
    "lemmatized = []\n",
    "\n",
    "tag_dict = {\"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV}\n",
    "\n",
    "for word, raw_tag in tagged:\n",
    "    tag = tag_dict.get(raw_tag[0].upper(), wordnet.ADV)\n",
    "    lemmatized.append(lemmatizer.lemmatize(word, pos = tag))\n",
    "                          \n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Define tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Funnction tokenize the input text.\n",
    "    \n",
    "    Input: text as string\n",
    "    \n",
    "    Output: List of words of cleaned text \n",
    "    \"\"\"\n",
    "    \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Normalization : \n",
    "    # Replace punctuations with \" \" and make string lowercase\n",
    "    # Replace all punctuations except apostrophes\n",
    "    text = re.sub(\"[^a-zA-Z0-9']\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenize: Split to words\n",
    "    tokenized = text.split(\" \")\n",
    "    tokenized = [word for word in tokenized if word != \"\" ]\n",
    "    \n",
    "    # Remove stop_words\n",
    "    cleaned = [word for word in tokenized if word not in stop_words]\n",
    "    \n",
    "    # Part of speech tagging\n",
    "    tagged = nltk.pos_tag(cleaned)\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatized = []\n",
    "\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    for word, raw_tag in tagged:\n",
    "        tag = tag_dict.get(raw_tag[0].upper(), wordnet.ADV)\n",
    "        lemmatized.append(lemmatizer.lemmatize(word, pos = tag))\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The message might be saying that they have been stuck in the presidential palace ( pal ) since the same Tuesday ( as the quake ). They need water. The message says they are not finding a little water. No names, no number of people given.\n",
      "Tokenized text: ['message', 'might', 'say', 'stick', 'presidential', 'palace', 'pal', 'since', 'tuesday', 'quake', 'need', 'water', 'message', 'say', 'find', 'little', 'water', 'name', 'number', 'people', 'give']\n",
      "---\n",
      "Original text: I live in La Plaine, my wife is pregnant, she got injured by falling blocks. She cannot walk. we have not found anyone to help us. We are in the area called Moya\n",
      "Tokenized text: ['live', 'la', 'plaine', 'wife', 'pregnant', 'get', 'injured', 'fall', 'block', 'cannot', 'walk', 'find', 'anyone', 'help', 'us', 'area', 'call', 'moya']\n",
      "---\n",
      "Original text: Our imcomprehensibley of 9 member was repatriated incomprehensible - (Location) Duvivier, Fond Grango\n",
      "Tokenized text: ['imcomprehensibley', '9', 'member', 'repatriate', 'incomprehensible', 'location', 'duvivier', 'fond', 'grango']\n",
      "---\n",
      "Original text: URGENT ACTION NEEDED --this is follow up from previous SMS in regards to possible delivery... possible to deliver my baby in an other country, does not matter what it is..\n",
      "Tokenized text: ['urgent', 'action', 'need', 'follow', 'previous', 'sm', 'regard', 'possible', 'delivery', 'possible', 'deliver', 'baby', 'country', 'matter']\n",
      "---\n",
      "Original text: Please add airtime on my phone. Thanks\n",
      "Tokenized text: ['please', 'add', 'airtime', 'phone', 'thanks']\n",
      "---\n",
      "Original text: We are in dire need of food, water and temporary shelter. We need to know where we can get supplies. We are located in Frere Boukan, La plaine.\n",
      "Tokenized text: ['dire', 'need', 'food', 'water', 'temporary', 'shelter', 'need', 'know', 'get', 'supply', 'locate', 'frere', 'boukan', 'la', 'plaine']\n",
      "---\n",
      "Original text: The road is damaged and there is no gas, would like to find a motocycle. It costs 500 to go to Port-au-Prince. No one has any money.\n",
      "Tokenized text: ['road', 'damage', 'gas', 'would', 'like', 'find', 'motocycle', 'cost', '500', 'go', 'port', 'au', 'prince', 'one', 'money']\n",
      "---\n",
      "Original text: I woul like to know if aide is only available in pap as the provinces where badly hit as well\n",
      "Tokenized text: ['woul', 'like', 'know', 'aide', 'available', 'pap', 'province', 'badly', 'hit', 'well']\n",
      "---\n",
      "Original text: We have a factory that is on FIRE on road to the airport near Sogebank. It's starting to burn several nearby houses with documents left in them. Please come and help us!!!\n",
      "Tokenized text: ['factory', 'fire', 'road', 'airport', 'near', 'sogebank', 'start', 'burn', 'several', 'nearby', 'house', 'document', 'leave', 'please', 'come', 'help', 'us']\n",
      "---\n",
      "Original text: Good morning Radio One, there are a lot of victims in Fonds Verettes with no one to speak for us\n",
      "Tokenized text: ['good', 'morning', 'radio', 'one', 'lot', 'victim', 'fonds', 'verettes', 'one', 'speak', 'us']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Check the function for 10 item in X: \n",
    "for each in X[100:110]:\n",
    "    print(\"Original text:\",each)\n",
    "    print(\"Tokenized text:\",tokenize(each))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Define ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultiOutputClassifier(AdaBoostClassifier())),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (18351,)\n",
      "Shape of X_test: (7865,)\n",
      "Shape of y_train: (18351, 35)\n",
      "Shape of y_test: (7865, 35)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3 )\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...mator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "#y_pred_train = pipeline.predict(X_train)\n",
    "y_pred_test = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline test score: 0.238143674507\n"
     ]
    }
   ],
   "source": [
    "# pipeline test score\n",
    "print(\"Pipeline test score:\",pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report with Test Data:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       0.79      0.97      0.87      5934\n",
      "               request       0.78      0.50      0.61      1332\n",
      "                 offer       0.00      0.00      0.00        36\n",
      "           aid_related       0.74      0.61      0.67      3219\n",
      "          medical_help       0.59      0.27      0.37       638\n",
      "      medical_products       0.65      0.31      0.42       418\n",
      "     search_and_rescue       0.53      0.17      0.26       192\n",
      "              security       0.17      0.03      0.05       144\n",
      "              military       0.59      0.33      0.43       245\n",
      "                 water       0.75      0.64      0.69       500\n",
      "                  food       0.83      0.66      0.73       878\n",
      "               shelter       0.76      0.56      0.64       705\n",
      "              clothing       0.61      0.30      0.40       115\n",
      "                 money       0.47      0.29      0.36       170\n",
      "        missing_people       0.23      0.03      0.06        92\n",
      "              refugees       0.60      0.30      0.40       260\n",
      "                 death       0.69      0.38      0.49       366\n",
      "             other_aid       0.53      0.13      0.21      1033\n",
      "infrastructure_related       0.44      0.10      0.17       505\n",
      "             transport       0.64      0.19      0.29       362\n",
      "             buildings       0.69      0.40      0.51       392\n",
      "           electricity       0.57      0.19      0.29       168\n",
      "                 tools       0.00      0.00      0.00        48\n",
      "             hospitals       0.24      0.10      0.14        78\n",
      "                 shops       0.00      0.00      0.00        28\n",
      "           aid_centers       0.17      0.04      0.06       103\n",
      "  other_infrastructure       0.36      0.08      0.13       341\n",
      "       weather_related       0.85      0.66      0.75      2163\n",
      "                floods       0.84      0.60      0.70       623\n",
      "                 storm       0.73      0.45      0.55       738\n",
      "                  fire       0.53      0.12      0.20        83\n",
      "            earthquake       0.88      0.77      0.82       702\n",
      "                  cold       0.74      0.35      0.48       171\n",
      "         other_weather       0.43      0.14      0.22       415\n",
      "         direct_report       0.71      0.39      0.50      1544\n",
      "\n",
      "           avg / total       0.72      0.58      0.62     24741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print classification report on train data\n",
    "#cls_report_train = classification_report(y_train, y_pred_train, target_names = df.iloc[:,4:].columns)\n",
    "#print(\"Classification report with Train Data:\\n\" ,cls_report_train)\n",
    "\n",
    "# Print classification report on test data\n",
    "cls_report_test = classification_report(y_test, y_pred_test, target_names = df.iloc[:,4:].columns)\n",
    "print(\"Classification report with Test Data:\\n\" ,cls_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of memory: None\n",
      "---------\n",
      "Parameters of steps: [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x7f6bf9398e18>, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('classifier', MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
      "           n_jobs=1))]\n",
      "---------\n",
      "Parameters of vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x7f6bf9398e18>, vocabulary=None)\n",
      "---------\n",
      "Parameters of tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
      "---------\n",
      "Parameters of classifier: MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
      "           n_jobs=1)\n",
      "---------\n",
      "Parameters of vect__analyzer: word\n",
      "---------\n",
      "Parameters of vect__binary: False\n",
      "---------\n",
      "Parameters of vect__decode_error: strict\n",
      "---------\n",
      "Parameters of vect__dtype: <class 'numpy.int64'>\n",
      "---------\n",
      "Parameters of vect__encoding: utf-8\n",
      "---------\n",
      "Parameters of vect__input: content\n",
      "---------\n",
      "Parameters of vect__lowercase: True\n",
      "---------\n",
      "Parameters of vect__max_df: 1.0\n",
      "---------\n",
      "Parameters of vect__max_features: None\n",
      "---------\n",
      "Parameters of vect__min_df: 1\n",
      "---------\n",
      "Parameters of vect__ngram_range: (1, 1)\n",
      "---------\n",
      "Parameters of vect__preprocessor: None\n",
      "---------\n",
      "Parameters of vect__stop_words: None\n",
      "---------\n",
      "Parameters of vect__strip_accents: None\n",
      "---------\n",
      "Parameters of vect__token_pattern: (?u)\\b\\w\\w+\\b\n",
      "---------\n",
      "Parameters of vect__tokenizer: <function tokenize at 0x7f6bf9398e18>\n",
      "---------\n",
      "Parameters of vect__vocabulary: None\n",
      "---------\n",
      "Parameters of tfidf__norm: l2\n",
      "---------\n",
      "Parameters of tfidf__smooth_idf: True\n",
      "---------\n",
      "Parameters of tfidf__sublinear_tf: False\n",
      "---------\n",
      "Parameters of tfidf__use_idf: True\n",
      "---------\n",
      "Parameters of classifier__estimator__algorithm: SAMME.R\n",
      "---------\n",
      "Parameters of classifier__estimator__base_estimator: None\n",
      "---------\n",
      "Parameters of classifier__estimator__learning_rate: 1.0\n",
      "---------\n",
      "Parameters of classifier__estimator__n_estimators: 50\n",
      "---------\n",
      "Parameters of classifier__estimator__random_state: None\n",
      "---------\n",
      "Parameters of classifier__estimator: AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "---------\n",
      "Parameters of classifier__n_jobs: 1\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# show parameters of pipeline\n",
    "for key, value in pipeline.get_params(deep = True).items():\n",
    "    print(\"Parameters of {}:\".format(key), value)\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...mator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'classifier__estimator__learning_rate': [0.7], 'classifier__estimator__n_estimators': [50], 'vect__ngram_range': [(1, 2)], 'vect__max_df': [0.5], 'tfidf__norm': ['l2'], 'tfidf__use_idf': [True], 'tfidf__sublinear_tf': [True]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create parameters for grid search\n",
    "parameters = {\n",
    "    \"classifier__estimator__learning_rate\": [0.7], # best parameter of [0.2, 0.5, 0.7]\n",
    "    \"classifier__estimator__n_estimators\":[50], # best parameter of [15, 35, 50]\n",
    "    \"vect__ngram_range\": [(1,2)], # best parameter of [(1,1),(1,2),(2,2)]\n",
    "    \"vect__max_df\": [0.5], # best parameter of [0.2, 0.5, 0.8]\n",
    "    \"tfidf__norm\": [\"l2\"], # best parameter of [\"l1\",\"l2\"]\n",
    "    \"tfidf__use_idf\": [True], # best parameter of [True, False]\n",
    "    \"tfidf__sublinear_tf\":[True] # best parameter of [True, False]\n",
    "}\n",
    "\n",
    "# initialize and fit grid search\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, scoring='f1_micro', n_jobs=-1)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__estimator__learning_rate': 0.7, 'classifier__estimator__n_estimators': 50, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': True, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.652130267305\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score:\", cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Classification report with Test Data:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       0.79      0.97      0.87      5934\n",
      "               request       0.75      0.53      0.62      1332\n",
      "                 offer       0.00      0.00      0.00        36\n",
      "           aid_related       0.76      0.59      0.66      3219\n",
      "          medical_help       0.58      0.21      0.30       638\n",
      "      medical_products       0.66      0.27      0.38       418\n",
      "     search_and_rescue       0.73      0.17      0.27       192\n",
      "              security       0.12      0.01      0.02       144\n",
      "              military       0.55      0.22      0.31       245\n",
      "                 water       0.74      0.68      0.71       500\n",
      "                  food       0.84      0.64      0.73       878\n",
      "               shelter       0.77      0.57      0.66       705\n",
      "              clothing       0.68      0.26      0.38       115\n",
      "                 money       0.52      0.22      0.31       170\n",
      "        missing_people       0.33      0.05      0.09        92\n",
      "              refugees       0.61      0.23      0.34       260\n",
      "                 death       0.81      0.34      0.48       366\n",
      "             other_aid       0.55      0.12      0.19      1033\n",
      "infrastructure_related       0.53      0.09      0.16       505\n",
      "             transport       0.72      0.15      0.25       362\n",
      "             buildings       0.75      0.32      0.45       392\n",
      "           electricity       0.60      0.18      0.28       168\n",
      "                 tools       0.06      0.02      0.03        48\n",
      "             hospitals       0.31      0.12      0.17        78\n",
      "                 shops       0.00      0.00      0.00        28\n",
      "           aid_centers       0.28      0.05      0.08       103\n",
      "  other_infrastructure       0.40      0.06      0.11       341\n",
      "       weather_related       0.87      0.62      0.72      2163\n",
      "                floods       0.86      0.58      0.69       623\n",
      "                 storm       0.75      0.49      0.59       738\n",
      "                  fire       0.76      0.23      0.35        83\n",
      "            earthquake       0.88      0.81      0.84       702\n",
      "                  cold       0.75      0.30      0.43       171\n",
      "         other_weather       0.51      0.11      0.18       415\n",
      "         direct_report       0.71      0.38      0.49      1544\n",
      "\n",
      "           avg / total       0.73      0.56      0.61     24741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report on test data\n",
    "y_pred_test_cv = cv.predict(X_test)\n",
    "\n",
    "cv_cls_report_test = classification_report(y_test, y_pred_test_cv, target_names = df.iloc[:,4:].columns)\n",
    "print(\"CV Classification report with Test Data:\\n\" ,cv_cls_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom transformer which calculates text lenght \n",
    "\n",
    "class text_length_extractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        extracted_lengths = pd.Series(X).apply(lambda x:len(x))\n",
    "        return pd.DataFrame(extracted_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pipeline \n",
    "pipeline = Pipeline([\n",
    "    (\"features\", FeatureUnion([\n",
    "        (\"nlp_pipeline\", Pipeline([\n",
    "            (\"vect\",CountVectorizer()),\n",
    "            (\"tfidf\",TfidfTransformer())\n",
    "        ])),\n",
    "        \n",
    "         (\"text_len_ext\", text_length_extractor())\n",
    "    ])),\n",
    "    \n",
    "    ('classifier', MultiOutputClassifier(AdaBoostClassifier()))         \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of memory: None\n",
      "---------\n",
      "Parameters of steps: [('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('nlp_pipeline', Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_r..., smooth_idf=True, sublinear_tf=False, use_idf=True))])), ('text_len_ext', text_length_extractor())],\n",
      "       transformer_weights=None)), ('classifier', MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
      "           n_jobs=1))]\n",
      "---------\n",
      "Parameters of features: FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('nlp_pipeline', Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_r..., smooth_idf=True, sublinear_tf=False, use_idf=True))])), ('text_len_ext', text_length_extractor())],\n",
      "       transformer_weights=None)\n",
      "---------\n",
      "Parameters of classifier: MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
      "           n_jobs=1)\n",
      "---------\n",
      "Parameters of features__n_jobs: 1\n",
      "---------\n",
      "Parameters of features__transformer_list: [('nlp_pipeline', Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])), ('text_len_ext', text_length_extractor())]\n",
      "---------\n",
      "Parameters of features__transformer_weights: None\n",
      "---------\n",
      "Parameters of features__nlp_pipeline: Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])\n",
      "---------\n",
      "Parameters of features__text_len_ext: text_length_extractor()\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__memory: None\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__steps: [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__tfidf: TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__analyzer: word\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__binary: False\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__decode_error: strict\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__dtype: <class 'numpy.int64'>\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__encoding: utf-8\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__input: content\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__lowercase: True\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__max_df: 1.0\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__max_features: None\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__min_df: 1\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__ngram_range: (1, 1)\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__preprocessor: None\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__stop_words: None\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__strip_accents: None\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__token_pattern: (?u)\\b\\w\\w+\\b\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__tokenizer: None\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__vect__vocabulary: None\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__tfidf__norm: l2\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__tfidf__smooth_idf: True\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__tfidf__sublinear_tf: False\n",
      "---------\n",
      "Parameters of features__nlp_pipeline__tfidf__use_idf: True\n",
      "---------\n",
      "Parameters of classifier__estimator__algorithm: SAMME.R\n",
      "---------\n",
      "Parameters of classifier__estimator__base_estimator: None\n",
      "---------\n",
      "Parameters of classifier__estimator__learning_rate: 1.0\n",
      "---------\n",
      "Parameters of classifier__estimator__n_estimators: 50\n",
      "---------\n",
      "Parameters of classifier__estimator__random_state: None\n",
      "---------\n",
      "Parameters of classifier__estimator: AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "---------\n",
      "Parameters of classifier__n_jobs: 1\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# show parameters of pipeline\n",
    "for key, value in pipeline.get_params(deep = True).items():\n",
    "    print(\"Parameters of {}:\".format(key), value)\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameters for grid search\n",
    "parameters = {\n",
    "    \"classifier__estimator__learning_rate\": [0.7], # best parameter of [0.2, 0.5, 0.7]\n",
    "    \"classifier__estimator__n_estimators\":[50], # best parameter of [15, 35, 50]\n",
    "    \"features__nlp_pipeline__vect__ngram_range\": [(1,2)], # best parameter of [(1,1),(1,2),(2,2)]\n",
    "    \"features__nlp_pipeline__vect__max_df\": [0.5], # best parameter of [0.2, 0.5, 0.8]\n",
    "    \"features__nlp_pipeline__tfidf__norm\": [\"l2\"], # best parameter of [\"l1\",\"l2\"]\n",
    "    \"features__nlp_pipeline__tfidf__use_idf\": [True], # best parameter of [True, False]\n",
    "    \"features__nlp_pipeline__tfidf__sublinear_tf\":[True] # best parameter of [True, False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('nlp_pipeline', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df...mator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'classifier__estimator__learning_rate': [0.7], 'classifier__estimator__n_estimators': [50], 'features__nlp_pipeline__vect__ngram_range': [(1, 2)], 'features__nlp_pipeline__vect__max_df': [0.5], 'features__nlp_pipeline__tfidf__norm': ['l2'], 'features__nlp_pipeline__tfidf__use_idf': [True], 'features__nlp_pipeline__tfidf__sublinear_tf': [True]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize and fit grid search\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, scoring='f1_micro', n_jobs=-1)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.652741760573\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score:\", cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Classification report with Test Data:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       0.82      0.93      0.87      5934\n",
      "               request       0.78      0.54      0.64      1332\n",
      "                 offer       0.25      0.03      0.05        36\n",
      "           aid_related       0.74      0.59      0.66      3219\n",
      "          medical_help       0.60      0.21      0.31       638\n",
      "      medical_products       0.73      0.26      0.38       418\n",
      "     search_and_rescue       0.67      0.18      0.28       192\n",
      "              security       0.13      0.01      0.03       144\n",
      "              military       0.58      0.27      0.37       245\n",
      "                 water       0.70      0.69      0.70       500\n",
      "                  food       0.81      0.70      0.75       878\n",
      "               shelter       0.80      0.50      0.61       705\n",
      "              clothing       0.72      0.30      0.42       115\n",
      "                 money       0.49      0.21      0.30       170\n",
      "        missing_people       0.75      0.13      0.22        92\n",
      "              refugees       0.66      0.27      0.39       260\n",
      "                 death       0.82      0.39      0.53       366\n",
      "             other_aid       0.55      0.14      0.22      1033\n",
      "infrastructure_related       0.48      0.07      0.12       505\n",
      "             transport       0.74      0.15      0.26       362\n",
      "             buildings       0.72      0.32      0.44       392\n",
      "           electricity       0.65      0.17      0.27       168\n",
      "                 tools       0.14      0.02      0.04        48\n",
      "             hospitals       0.37      0.13      0.19        78\n",
      "                 shops       0.00      0.00      0.00        28\n",
      "           aid_centers       0.44      0.07      0.12       103\n",
      "  other_infrastructure       0.50      0.04      0.08       341\n",
      "       weather_related       0.86      0.62      0.72      2163\n",
      "                floods       0.87      0.59      0.70       623\n",
      "                 storm       0.77      0.42      0.54       738\n",
      "                  fire       0.80      0.19      0.31        83\n",
      "            earthquake       0.88      0.79      0.84       702\n",
      "                  cold       0.68      0.28      0.40       171\n",
      "         other_weather       0.52      0.09      0.15       415\n",
      "         direct_report       0.74      0.44      0.55      1544\n",
      "\n",
      "           avg / total       0.75      0.56      0.61     24741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report on test data\n",
    "y_pred_test_cv = cv.predict(X_test)\n",
    "\n",
    "cv_cls_report_test = classification_report(y_test, y_pred_test_cv, target_names = df.iloc[:,4:].columns)\n",
    "\n",
    "print(\"CV Classification report with Test Data:\\n\" ,cv_cls_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pick  = pickle.dumps('cv.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
